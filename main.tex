%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed



\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{balance}
%\usepackage{amsxtra}
%\usepackage{amsfonts}
\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{float}
\usepackage{graphicx}
%\usepackage{caption}
%\usepackage{setspace}
\usepackage{color}
\usepackage{subfigure}
%\usepackage{subfig}
%\usepackage{psfrag}
%\usepackage{dsfont}
\usepackage{multirow}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{todonotes}
\usepackage{url}
\usepackage{hyperref}
\usepackage{bm}


\newcommand{\Ne}{\mathbb {N}}

\usepackage{hyperref}



% *** MATH PACKAGES ***
%
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath



\graphicspath{ {fig/} }
%\usepackage{subfigure}
% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage[linesnumbered,ruled]{algorithm2e}
\hyphenation{op-tical net-works semi-conduc-tor}

\newtheorem{problem}{Problem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newcommand*{\affaddr}[1]{#1} % No op here. Customize it for different styles.
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\texttt{#1}}

\newcommand\NB[1]{$\spadesuit$\footnote{NB: #1}}
\newcommand\JL[1]{$\diamondsuit$\footnote{MH: #1}}
\newcommand\ME[1]{$\spadesuit$\footnote{ME: #1}}


\begin{document}

\title{Cyber-Physical Attack Intention Prediction and Recovery}


%\author{\IEEEauthorblockN{Mahmoud Elnaggar\affmark[1],
% and Nicola Bezzo\affmark[1,2]}
%\IEEEauthorblockA{\affmark[1]Department of Systems and Information Engineering\\
%\affmark[2]Department of Electrical and Computer Engineering\\
%University of Virginia\\
%Email: \{mae3tb,nbezzo\}@virginia.edu}}


\author{Mahmoud Elnaggar$^{1}$ and Nicola Bezzo$^{1,2}$% <-this % stops a space
%\thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$Department of Electrical and Computer Engineering}
\thanks{$^{2}$Department of Systems and Information Engineering
        {\tt\small  mae3tb@virginia.edu, nbezzo@virginia.edu}}%
}


\maketitle
%\thispagestyle{empty}
%\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Sensor spoofing attacks that hide within the uncertainty profiles of autonomous cyber-physical system (CPS) dynamics can drag the system to undesired dangerous states. Using state-of-the-art CPS security techniques is not sufficient to detect which sensors are being spoofed and fail to protect the system from such an attack. We propose an Inverse Reinforcement Learning technique that leverages Bayesian Inference and uses the history of sensor readings and control actions of a compromised autonomous (CPS) to predict the goal of the attacker and determine which set of sensors are compromised. We show that by predicting this information, we recover the system from the stealthy attack.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%\NB{iPlease use the correct ACC template}
%\NB{I rewrote the intro. It was very confusing and it looks more a news report}
The majority of modern cyber-physical systems (CPSs) are not built with cybersecurity in mind. The tight coupling between information technology and the physical world have introduced security vulnerabilities in the form of physical and cyber attacks (e.g., sensor, actuator, controller, communication and environment attacks).
%Cyber-Physical security has grasped a lot of attention in the research community specially during the last decade. 
Multiple incidents that have occurred recently indicate the ability of a malicious attacker to compromise modern cyber-physical systems using different techniques. For example, the well known StuxNet cyber attack on an Iranian nuclear reactors in 2011 was performed by exploiting five zero-day vulnerabilities in the Operating Systems installed on the computers that operate the SCADA system to control the nuclear centrifuges \cite{langner_2013}. This type of attack operated at a cyber level targeting the physical plant by leveraging the knowledge on how the system was being operated and controlled. Cyber-attacks on modern automobiles have also been widely explored like in \cite{} in which the hackers were able to take over different  system components and hijack the vehicle by tempering with its infotainment system.
%Another type of attacks on a cyber-physical system is the one that operates on the system control level, with the aim to exploit vulnerabilities in the system by compromising different components of the control system.

A typical CPS contains multiple buses/networks that connect sensors and actuators with controllers, data storage and processing units, and human machine interfaces. More precisely, it consists of: a) controllers, b) networking devices and buses, c) sensors, s) actuators, and d) the physical plant. An attacker can compromise the operation of a CPS by attacking any of these components. 

In this paper we focus on sensor attacks, specifically sensor spoofing attacks in which a malicious program masquerades as another sensor by falsifying data while staying stealthy within the error noise of the sensor measurement. GPS spoofing in particular has been demonstrated in several works \cite{humphreys2008assessing, protecting_gps_2016, peterson_faramarzi_2011}. A malicious attacker can use a hand-held device to deceive a GPS receiver by broadcasting signals synchronized with the genuine signals observed by the target receiver. The power of the counterfeit signals is then gradually increased and drawn away from the genuine signals \cite{humphreys2008assessing}. Such attack has been demonstrated on different vehicles compromising their safety and/or hijacking them to undesired states.
%
%If this type of attack is exploited on a dynamic CPS like an unmanned ground vehicle (UGV) or an unmanned aerial vehicle (UAV) that is navigating to a desired destination, it can make the autonomous vehicle crash or get hijacked and sent to an undesired location.
%
%One of the most widely used sensors for localization is the GPS. It has been shown in \cite{humphreys2008assessing} that a typical GPS is vulnerable to sensor spoofing attacks. A malicious attacker can use a hand-held device to deceive the GPS receiver in a system by broadcasting incorrect GPS signals. 
%If this type of attacks is performed on a dynamic cyber-physical system i.e. a UGV (Unmanned Ground Vehicle) or a UAV (Unmanned Aerial Vehicle) that is navigating to a desired destination, it can make the autonomous vehicle crash or get hijacked and sent to an undesired location
For example, it is believed that GPS spoofing leaded to the capturing of a Sentinel drone in Iran in 2011 \cite{peterson_faramarzi_2011}. More recently, researchers in \cite{protecting_gps_2016} have also demonstrated a GPS spoofing attack on a vessel, making it deviate from its desired course.
 
%An example that shows how the impact of GPS spoofing can be drastic is the American UAV hijacking incident in 2011. According to \cite{peterson_faramarzi_2011} the autonomous drone was supposed to land in Afghanistan and the attacker sent false positions to the drone via GPS spoofing to deceive the autonomous controller and make it land in Iran instead of Afghanistan. The US government acknowledged losing the control of the drone but it didn't confirm whether it happened due to a GPS spoofing attack or not. Another example of GPS spoofing attacks was demonstrated by researchers from University of Texas and Cornell University, when they conducted an attack by spoofing the GPS receiver on a \$80 million vessel cruising in the Mediterranean sea. The attack made the vessel go off the desired course while thinking it is still following the correct coarse \cite{protecting_gps_2016}.

\begin{figure}[t]
\centering
\includegraphics[width=0.4\textwidth]{problem}
\caption{A malicious attacker spoofs UAV's  GPS receiver to drive it to an undesired destination instead of its original one}
 \label{fig:problem}
\end{figure}
\NB{figure 1 is too big.}

In this paper, we are interested in addressing similar problems as the one illustrated in Fig.\ref{fig:problem}. We consider the case where an autonomous vehicle, equipped with multiple sensors, is tasked to perform a go-to-goal navigation. 
%The state estimator of the autonomous vehicle uses a number of sensors to estimate its position. 
%We assume that the vehicle has at least one sensor that is not vulnerable to spoofing attacks or in other words, the attacker is not able to spoof at least one sensor. 
A malicious attacker performs a coordinated attack by spoofing one or more of sensors on the vehicle. The goal of the attacker is to drive the vehicle to a different destination than its original destination while hiding within the sensor and actuator noise profile and disturbance model of the system. 
%The attacker's desired destination can be a hostile territory to the vehicle, or an obstacle that will make the vehicle crash if it hits it. 
%We consider a smart attacker who hides his sensor attack vectors within the noise profiles of the attacked sensors or within the environment disturbance model.

Our goal is to develop a technique to infer the intention of the attacker and recover the system before reaching the undesired state. 
The idea is that if we are able to determine which set of sensors are under attack before the vehicle arrives to the undesired destination, we can correctly reconstruct the state of the vehicle using the remaining uncompromised set of sensors.
We show that by using Inverse Reinforcement Learning techniques we are able to:
\begin{itemize}
    \item Predict the intention of the attacker.
    \item Determine the set of compromised sensors.
    \item Recover the correct state of the vehicle and continue its operation. 
\end{itemize}
%\NB{Check.you need to add the approach that we are using and contribution here.}
%Once the vehicle state is estimated correctly, the controller can drive the vehicle back on its coarse navigating towards the desired destination.

\subsection{Related Work}\label{subsec:related}
%\NB{Check. related work needs to be rewritten in a different way without subsections. You need to introduce our proposed work before talking about related work in IRL. The first part of the related work needs to be expanded. Do not condensate multiple papers together. Expand each paper description. After that introduce that in this work we will use machine learning techniques and that they have not been explored too much in the literature on CPS cyber-security. Refer to my ACC paper on ROMPD and then describe related work on RL and IRL.}

Various efforts have been done by members of the cyber-physical systems research community to guarantee the safety of cyber-physical systems under sensor attacks. A major contribution was done by developing secure and resilient state estimators. The results presented in \cite{Fawzi2014,Ivanov2014,Pajic2014,Pajic2017} \NB{reference 7 in the citation needs to be redone...repeated names and not the right format. Use google scholar to get the bib file for each reference} show that by using redundant sensors in a system, and leveraging the knowledge of the system model, the secure state estimators are able to correctly estimate the state of the system if the number of attacked sensors is less than half of the number of the sensors. An important assumption that these works considered is that the sensor attack vectors don't hide within the noise profiles of the sensors nor the environment disturbance model. Given this assumption, the resilient state estimator can not only correctly estimate the state of a compromised system, but also it can tell which sensors are under attack and which are not by comparing sensor readings with system model. \NB{as mentioned before, we need to expand more this part and show other works on CPS cyber-security}

The problem of performing motion planning for autonomous vehicles under sensor attacks is addressed in \cite{bezzo2016stochastic} in which the authors considered UAVs subject to malicious attacks driving the vehicle to unsafe states. They introduced the notion of Redundant Observable Markov Decision Process (ROMDP) which is a class of Markovian processes that deals with redundant attacked sensor measurements. Resiliency against attacks is acheieved by properly selecting the reward function thus avoiding actions that could lead to hijacking the vehicle to undesired regions in the state space.

In our case, we assume that the attacker hides his attack vectors within the noise profiles of the system sensors, which means we need to develop a new technique to detect which set of sensors are under attack after we detect that an attack is happening. We leverage state-of-the-art Inverse Reinforcement Learning (IRL) techniques to infer the desired destination of the attacker and at the same time detect which sensors are compromised.

The problem of Inverse Reinforcement Learning has been addressed in multiple domains. It was first introduced in \cite{Ng2000}. An algorithm for apprenticeship learning via IRL was introduced in \cite{Abbeel2004a} where the authors considered the case where the reward function is expressible as a linear combination of known features. IRL problem was first cast as a Bayesian inference problem in  \cite{Ramachandran2007}. It was shown that there Bayesian Inverse Reinforcement Learning (BIRL) algorithm can be used for reward learning and also policy learning. Policy walk algorithm was also introduced which is based on Monte Carlo Marcov Chain (MCMC) algorithm. Michini et al. \cite{Michini2015,Michini2013,Michini2012} extended BIRL algorithm to deal with continuous state and action spaces and introduced the Non-Parametric Bayesian Inverse Reinforcement Learning (BNIRL) algorithm. They  showed that by observing a UAV performing different maneuvers in a continuous 3D space. The BNIRL algorithm was able to infer what are the intermediate goals and the final goal that the observed UAV was trying to reach. They used this predicted information to make another UAV perform the same observed maneuvers.

The previous works considered the case where we can observe a moving agent completing its desired trajectory from source to destination. In other applications, we may want to observe the agent navigating in the environment and try to predict its future trajectory and its desired destination. In this case, we have partial set of observation. This problem was studied in \cite{Best2015} where they used Bayesian Inference to predict the intention of an agent navigating in a partially occluded environment. The intention of the agent was represented by the goal the agent is trying to reach based on the assumption that the agent will most likely take the shortest path to the goal position, with some uncertainty in its transition model.

In the cyber-physical literature, to our knowledge, no research has been done so far that uses IRL to predict the intention of an attacker.

The rest of the paper is organized as follows: In section \ref{sec:Preliminaries}, we describe the theoretical foundation of our approach. In section \ref{sec:approach}, we provide details of our approach. In section \ref{sec:simulations}, we illustrate the simulations we ran to test our approach. We describe the experiments we conducted and show their results in section \ref{sec:exp}. Finally, we conclude our work in section \ref{sec:conclusion}.
%The idea of check-pointing and recovery has been applied to many computing systems \cite{Zhang2003,Processors2017,Koo1987, Prakash1996}. By taking snapshots of the state of a system in a cyclic fashion or after the occurrence of specific events, we can roll-back to the last saved checkpoint if a fault occurs to recover the system from that fault. This technique cannot be directly applied to a dynamical system without modifying the definitions of a checkpoint and a roll-back for two reasons. 
%\begin{enumerate}
    %\item This technique assumes that every checkpoint we take represents a correct state of the system. For a dynamical system under sensor attack, detection of an attack doesn't occur immediately after an attack starts. Which means that the saved checkpoints between the time the attack started and the time it is detected are considered incorrect states. For that matter, we need to consider the fact that we can have correct and incorrect checkpoints in the system and the the recovery mechanism shall find the last correct checkpoint in the system.
    %\item Roll-back is considered successful when the last checkpoint is recovered. For a dynamical system which moves in the physical space. To tolerate a fault or mitigate an attack, the recovery process is considered success-full when the correctly estimate the state of the system, not necessarily by returning back to the last saved correct check-point. Thus, the role of the recovery mechanism in our case is to find what is the last correct checkpoint, and use this information to determine the current state of the system correctly.
%\end{enumerate}

\section{Preliminaries}\label{sec:Preliminaries}
In this section, we provide the foundations from MDP, RL, and IRL that we use to develop our approach in this work.
\subsection{Markov Decision Process}
A finite state Markov Decision Process (MDP) \cite{puterman2014markov} consists of a tuple $(S,A,T,R,\gamma)$ where:
\begin{itemize}
    \item $S$ is the set of all the states.
    \item $A$ is the set of all possible actions.
    \item $T : S\times A\times S \mapsto [0,1]$ is the transition probabilities function. $T(s,a,s')$ is the probability that moving to state $s'$ from state $s$ after action $a$ is taken.
    \item $R$ is the reward function which can be a function of the state only $R(s)$, a function of states and actions $R(s,a)$, or a function of the states, actions, and the next states $R(s,a,s')$. For the rest of the paper, we will use $R(s)$ to denote the reward function without loss of generality.
    \item $\gamma$ is the discount factor that the reward function gets discounted with over time.
\end{itemize}
To solve and MDP problem, we are interested in finding an optimal stationary policy $\pi^*: S \mapsto A$ that maximizes the expected value of the discounted future rewards.
In order to find $\pi^*$, we need to define a value function for all state $s \in S$ given a reward function $R(s)$ and a policy $\pi$ as follows:
\[ V^\pi(s,R) = R(s) + \sum_{s'}^{}\gamma\,T(s,\pi(s),s')\,V^\pi(s') \]
The value function of state $s$ is equal to the reward received in state $s$ plus the sum of the discounted expected value function for each neighbor state $s'$ when actions are taken according to the policy $\pi$.
To evaluate the value of taking action $a$ for each state $s$, we define the state-action value function which is called the Q-function.
\[Q^\pi(s,a,R) = R(s) + \sum_{s'}{} \gamma\,T(s,a,s')\,V^\pi(s') \]
Then, a policy $\pi$ is optimal iff, for each state $s \in S$:
\[ \pi(s) = \arg\!\max_{a\in A} Q^\pi(s,a,R)\]
The optimal policy $\pi^*$ corresponds to a state value function $V^*$ and a state-action value function $Q^*$.

\subsection{Reinforcement and Inverse Reinforcement Learning}
Reinforcement Learning provides the notion of an agent that navigates the states of MDP environment by taking and an action $a \in A$. The agent receives a reward $r$ when it transitions from a state $s$ to state $s'$ by taking action $a$. The task of Reinforcement Learning algorithms is to learn the optimal policy $\pi^*$ that maximizes the future expected rewards for the agent from its history of interaction with the environment. The history of the interactions can be formulated as a set of the tuple $\big \langle s,a,r,s' \big \rangle$.

Q-learning is a model-free reinforcement learning technique. It relies on learning the state-action value function for each state-action pair. By doing so, the optimal policy can be constructed by simply choosing the action that has the highest value at each state. The core of the iterative learning method does state-action value iteration update as follows:
\begin{equation}
    Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha ( r_t + \gamma \max_{a} Q(s_{t+1},a) - Q(s_t,a_t))
\end{equation}
Where $\alpha$ is the learning rate, and $r_t$ is the reward observed at the current state.

In Inverse Reinforcement Learning (IRL), we consider an agent acting optimally or sub-optimally in an MDP environment. The agent is following an optimal or sub-optimal policy $\pi^a(s)$. The policy of the agent $\pi^a(s)$ can be observed as a set of state-action pairs $\{(s_1,a1),(s_2,a_2), ...,(s_N,a_N)\}$, where $N$ is the number of observations. The goal of IRL is to infer the reward function $R$ that is responsible for generating these observations.%\NB{Check.both this and the previous sections are too brief. Either we add some mathematical formulation that will be used next or we may have to remove or combine with another section}

\section{Problem Statement}\label{sec:problem}
We consider an autonomous vehicle that performs a navigation task to a desired destination in a stochastic environment. The vehicle is equipped with $N$ number of sensors. The readings from the sensors are fused together to estimate the state of the vehicle. We also consider a malicious attacker that can spoof a subset of sensors $\mathcal{S'}$ where $|\mathcal{S'}| < N$. The goal of the attacker is to drive the vehicle towards an undesired destination while hiding its attack vectors within the noise profiles of the spoofed vehicle sensors or hiding it within the disturbance model of the stochastic environment.  Fig. \ref{fig:sensor_spoofing} summarizes the situation in which a sensor spoofing attack stealthily begins at $t_1$. The attack can stay stealthy to the system until the noise margins of the spoofed sensors don't overlap anymore with the noise margins of the uncompromised ones at $t_2$ as shown in Fig. \ref{fig:sensor_spoofing}. From $t_2$, an attack or fault can be detected due to the divergence in sensor readings. At this point, it is hard to distinguish between the compromised and uncompromoised sensors, as all sensor readings comply with the uncertainty model of the vehicle dynamics and the environment. .
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{sensor_spoofing}
\caption{An attack on the blue sensor starts at $t_1$. The attack can be detected at $t_2$ when the error margins of the blue sensor (compromised) and the orange sensor (uncompromised) don't overlap anymore.}
 \label{fig:sensor_spoofing}
\end{figure}

Formally, We are interested in solving the following problem:
\begin{problem}\label{prob:p1}
Attacker Intention Inference and Recovery: Consider the set $\mathcal{G}$ is the set of states that are considered undesired. Consider a super-set of observations $\mathcal{O}_{t_2:t} = \{\mathcal{O}_1, \mathcal{O}_2, ..., \mathcal{O}_N\}$, where $\mathcal{O}_{i,t_2:t} = \{(s_{i,t_2}, a_{t_2}), (s_{i,t_2+1}, a_{t_2+1}), ..., (s_{i,t_2+t}, a_{t_2+t})\}$ is the finite set of measurement-action pairs for each sensor $i$ which includes the history of sensor data recorded starting from the beginning of the attack detection time  $t = t_2$ until the current time $t$. We assume the attacker's goal is to drive the vehicle towards a target goal $g^a \in \mathcal{G}$. Where, $\mathcal{G}$ is the set of all possible undesired goals known a priori. Our goal is to recover from this attack. In order to do so, we need to infer the intention of the attacker by estimating their desired goal $g^a$ and determine the subset of compromised sensors $\mathcal{S'}$. \NB{Problem needs to be rewritten more formally. Given a set of undesired goals...a set of observations/actions ... and assume that a subset of observations...is compromised at a certain time t, find a policy to predict the intended goal of the attack and recover the system}
\end{problem}

\section{Attacker Intention Inference}\label{sec:approach}
We propose to cast Problem \ref{prob:p1} as an Inverse Reinforcement Learning problem. In this problem, we consider that the autonomous vehicle follows an optimal policy $\pi*$ to perform its navigation task to the desired goal. We call $\pi*$ the mission policy. After the attacker starts his attack on the vehicle, we consider the vehicle starts acting as an agent that navigates in the environment according to the attacker's desired policy $\pi^a$. We call $\pi^a$ the attack policy. The policy $\pi^a$ drives the agent towards the target destination $g^a$ as fast as possible while hiding the attack within the sensors noise profiles or the environment disturbance model. At each state $s$, the policy $\pi^a$ is calculated according to:
\[ \pi^a(s) = \arg\!\max_{a\in A} Q^{\pi^a}(s,a,R^a)\]

The reward function $R^a$ can take many forms. For the sake of simplicity we assume that the reward function takes the following form . 
  \begin{equation}
    R^a(s)=\left\{
                \begin{array}{ll}
                  C\hspace{2em} s = g\\
                  -\epsilon\hspace{1.6em} s \ne g
                \end{array}
              \right.
  \end{equation}
Where $C$ and $\epsilon$ are positive constants, the value of $C$ is much larger than the value of $\epsilon$. This reward function gives a small negative penalty for each action the vehicle takes (which can model the vehicle battery charge loss for example) and it gives a very high reward if the vehicle reaches the goal.
\subsection{Bayesian Inference Reinforcement Learning}
Given the set of observations $\mathcal{\mathcal{O}}_{t_2:t}$, the goal is to infer the intention of the attacker by inferring the reward function $R(s)$ the vehicle is trying to maximize. As attacker can compromise only a subset of sensors $\mathcal{S}'$, we rely on this assumption to infer the intention of the attacker. We use the history of past actions/sensor measurements to compare multiple optimal policies for each potential goal of the attacker to infer the intention. We propose a method that examines all the sensor readings and vehicle actions at the time the attack is detected $t_2$. The goal is to search for the subset of the compromised sensors $\mathcal{S}'$  that cause the vehicle to drift towards the bad goal. The algorithm we propose leverages Bayesian Inverse Reinforcement Learning techniques (BIRL)\cite{Ramachandran2007} in order to do the inference. In BIRL, the inference of the reward function posterior $Pr(R|\mathcal{O})$ is a function of the reward function prior $Pr(R)$ and the likelihood of the observations $Pr(\mathcal{O}|R)$ as follows:
\begin{equation} 
    Pr(R|\mathcal{O}) = \frac{Pr(\mathcal{O}|R)Pr(R)}{Pr(\mathcal{O})}
\end{equation}
Where, $Pr(\mathcal{O})$ is the probability distribution of $\mathcal{O}$ over the entire space of reward function $R$. The normalization constant $Pr(\mathcal{O})$ is usually hard to compute as it requires performing multiple integral calculations which is usually not feasible to compute analytically. 

To estimate the mean of posterior distribution while avoiding calculating $Pr(\mathcal{O})$, we leverage the MCMC sampling algorithm presented in \cite{andrieu2003introduction} that allows to estimate of the distribution of the posterior $Pr(\tilde{R}|\mathcal{O})$. Since this is an estimation problem, we are interested in finding the value of $R$ that minimizes the expected Least Square Error (LSE) of the estimate $E[(R-\tilde{R})^2]$. It was proven in \cite{Ramachandran2007} that the mean of the estimated posterior distribution is the optimal estimator that we can use.

In order to estimate the posterior $Pr(R|\mathcal{O})$ using MCMC algorithm, we need to compute two terms in each iteration. The first term is the prior $Pr(R)$ and the second one is the likelihood of the observations $Pr(\mathcal{O}|R)$.
The prior can be initialized according to our previous knowledge of the posterior distribution. If we don't have a prior knowledge about the posterior distribution, we can assume that the prior has the form of a uniform distribution.
In our case, we want to calculate ,for each sensor $i$, the likelihood of observing a set of sensor readings and actions taken during a period $T$ follow an optimal policy of the agent to go to a potential goal $g_j$. We can compute this calculation by comparing, at each observed sensor reading, the Q-function value of the action taken at this state with the Q-function value all possible actions that the agent can take at this state. Formally, we can calculate the likelihood of the observations as follows:
\begin{equation}
Pr(\mathcal{O}_{i,T} | g = g_j)  = \frac{e^{\alpha\sum_{T}{Q^*(s_{i,t},a_t,R_j)}}}{\sum_{b\in A}{}e^{\alpha\sum_{T}{}Q^*(s_{i,t},b,R_j)}}
\label{eqn:likelihood}
\end{equation}
where, $\alpha$ is a factor that indicates how much we are confident that the agent is acting optimally. This calculation requires solving an MDP problem to compute $Q^*$ for each state-action pair by performing Q-learning iterations as described in section \ref{sec:Preliminaries}.




%as follows:.
%
%\NB{here you have to show this algorithm or the math behind it with the notation used in our problems, otherwise this part makes little sense and the reviewer will be puzzled on how you are solving the problem if you don't show the math behind the approach} The output of MCMC is an estimate of the distribution of the posterior $Pr(\tilde{R}|O)$. Since this is an estimation problem, we are interested in finding the value of $R$ that minimizes the expected Least Square Error (LSE) of the estimate $E[(R-\tilde{R})^2]$. It was proven in \cite{Ramachandran2007} that the mean of the estimated posterior distribution is the optimal estimator in that case.



In order to infer the target goal of the attacker $g_a$ and the set of the compromised sensors $\mathcal{S}'$, we assume, initially, that the prior for the target goal is uniformly distributed among all possible undesired goals. We run MCMC iterations in order to calculate the mean of the attacker's target goal posterior. The number of the MCMC iterations is determined by the parameter ${e}_{max}$. We obtain a sample $g_j$ from the prior distribution of the undesired goals set $\mathcal{G}$. Then, we iterate over all the observations $\mathcal{O}_{i,T}$ we observed over a period of time $T$ for the sensor $i$. We update the posterior of the potential sampled goal $g_j$ from each observation by calculating the likelihood of the taken actions. The likelihood of taking action $a_t$ at state $s_{i,t}$ to go to goal $g_j$ can be directly computed as the action-value function (Q-function) for taking this action from this state:

\begin{equation} Pr(a_t|s_{i,t},g_j) = Q^*(s_{t,k},a_t,R_j)
\label{eqn:action_likelihood}
\end{equation}

Then, we compute the posterior that the goal $g_i$ is the target goal $g_a$ given the set of observations $\mathcal{O}_{i,t_2:t}$ as follows:
\begin{equation} Pr(g=g_a|\mathcal{O}_{i,t_2:t}) \propto Pr(\mathcal{O}_{i,t_2:t} | g = g_a) \\ \times Pr(g=g_a|\mathcal{O}_{i,t_2:t-1})
\label{eqn:goal_posterior}
\end{equation}

The first term on the right hand side of the equation is the likelihood of the observations given that the goal $g$ is the target goal $g_a$ which can be computed according to \ref{eqn:likelihood}. The second term is the prior which is updated from the posterior calculated at the previous time $t-1$.
After the MCMC iterations are done, we evaluate the level of confidence that the estimated posterior for a potential goal $g_t$ is the actual target goal $g_a$. We perform this by calculating the mean of the posterior. The higher the value of the posterior mean, the higher the confidence we have about our estimation. Algorithm \ref{alg:alg1} shows the steps we take to perform attacker's intention inference.

To start the recovery process, we compare the variance of posterior  to a threshold value $h$, and trigger the recover process if  it is less than the threshold value. The idea behind the trigger of the recovery process is that the readings we obtain from the uncompromised sensors will provide a posterior estimate of an undesired goal with high confidence which leads to low variance in the estimated posterior. Once the the set of the uncompromised sensors is determined, the rest of the sensors are considered to be compromised. The recovery process is performed by correctly estimating the state of the system by fusing the uncompromised sensors only and discarding the compromised ones. Once, the correct state of the system is estimated, the autonomous controller will be able correct the coarse of the vehicle to navigate toward its desired destination.
\begin{algorithm}\label{alg:alg1}
    %\SetKwInOut{Input}{Input}
    %\SetKwInOut{Output}{Output}
    \underline{AIP} ($\mathcal{O}, \mathcal{G}$)\;
    %\Input{Two nonnegative integers $a$ and $b$}
    %\Output{$\gcd(a,b)$}
    \ForEach {sensor $i \in \mathcal{S}$}
    {
        \While{iteration $e < e_{max}$}
        {
            Sample goal $g_j \in \mathcal{G}$\;
            \ForEach{Observation $(s_{i,k}, a_k) \in \mathcal{O}_i$}
            { 
                $Pr(a_k|s_{i,k},g_j) \leftarrow Q^*(s_{i,k},a_k,R_j)$\;
            }
            $Pr(\mathcal{O}_{i,T}|g=g_j) \leftarrow$  from \ref{eqn:likelihood}\;
            $Pr(g=g_j|\mathcal{O}_{i,T}) \leftarrow$ from \ref{eqn:goal_posterior}\;
        }
        $g_m(i) \leftarrow mean(Pr(g=g_j|\mathcal{O}_i))$\;
        $g_v(i) \leftarrow  var(Pr(g=g_j|\mathcal{O}_i))$\;
        \If{$g_v < h$}
        {
            $g_a \leftarrow g_m(i)$\;
            $i \in \mathcal{S}'$\;
        }
    }
    \caption{Attacker Intention Prediction}
\end{algorithm}

\subsection{Active Exploration}
The speed of convergence of the inference we perform in Algorithm \ref{alg:alg1} depends on which states the vehicle visited. If most of the states inside the the set of observations are associated with mission policy's actions similar to the attacker policy's actions, the convergence of the inference algorithm will be slower and the level of confidence of the inferred posterior may not reach the desired threshold value. We consider these states as insensitive states. On the other hand, the set of observations which contains mostly states in which there is a discrepancy between the action the vehicle takes according to the mission policy and the action the vehicle takes according to the attacker policy, will lead to faster convergence. We consider these states sensitive states. For Algorithm \ref{alg:alg1} to achieve convergence to the attacker's goal, it is clear that we want to collect a set of observations having as many sensitive states as possible.

 In most Inverse Reinforcement Learning problems, we don't have control over the optimal agent, and we are constrained by the set of observations we get by recording the agent acting optimally. Thus, in order to solve this problem in this work we propose an active learning policy in which we exploit the fact that the autonomous controller of the vehicle still has control of the vehicle while being under attack by the malicious attacker. This means that if we know which states are sensitive, we can make the vehicle explore more sensitive states by taking actions that lead towards sensitive states and not necessarily towards the desired goal. Knowing which states are sensitive and which are not is a hard problem. We propose an approach that, at each time step, searches for the action which moves the vehicle to a state that gives the highest discrepancy between the mission policy all potential attacker policies weighted by their posterior estimates.

We describe our Active Exploration approach in Algorithm \ref{alg:alg2}. At each time step $k$, the algorithm chooses the next action to apply $a^*$ by looking into each possible combination of sensor reading $s_i$, and next state $s'_i$. It picks the action that maximizes the probability of making an action different than the optimal action in the next state. A discrepancy factor  $W[a]$ for each <action, next state, goal> tuple is calculated based the transition probability $T$ to go this next state $s'_i$ from $s_i$, and estimated bad goal $g_m(i)$ for sensor $i$ at the current time step $k$, and how much we are confident that this estimate is the bad goal $g_v(i)$. The posterior estimate $g_m(i)$, and the variance $g_v(i)$ are calculated in Algorithm \ref{alg:alg1} in the same time step.

We apply $a^*$ at each time step with a probability $\delta, 0 < \delta < 1$ that decreases over time. The exploration factor $\delta$ indicates how much the inference algorithm will apply an action according to Active Exploration algorithm than applying an action according to the mission policy. The goal of doing Active Exploration is to make Attacker Intention Inference algorithm converge to the attacker's goal before the vehicle gets close to the attacker's desired region.
\begin{algorithm}\label{alg:alg2}
    %\SetKwInOut{Input}{Input}
    %\SetKwInOut{Output}{Output}
    \underline{AE} ($\mathcal{O}, \mathcal{G}$)\;
    %\Input{Two nonnegative integers $a$ and $b$}
    %\Output{$\gcd(a,b)$}
    Initialize W[$|A|$] with zeros\;
    \ForEach {action $a\in A$}
    {
        \ForEach {sensor $i \in \mathcal{S}$}
        {
            \ForEach {state $s'_i$}
            {
                \If{$\pi^{g_m(i)}(s'_i) \ne \pi^*(s'_i)$} 
                {                     $W[a]+= g_v(i) \times T(s_i, a, s'_i)$\;
                }
            }
        }
    }
    $a^*=\arg\!\max_{a\in A}W$\;
    \caption{Active Exploration}
\end{algorithm}
\subsection{Problem of large continuous state space}
In this work, we discretize the environment and come up with a set of discrete state-space and actions. As the size of the environment gets bigger, it can be very computationally expensive to discretize state space and actions which will result in a tremendously sized discrete state space and actions. There are various methods proposed to avoid the discretization step and do actions likelihood approximation instead. In \cite{Michini2013}, the authors demonstrated that they can do actions likelihood approximation by comparing the observed actions with actions generated by a simple closed loop controller. Similarly, we can use the same technique if we consider a continuous state space in our case. First, we record the action $a_i$ corresponding to each sensor reading $s_i$ to obtain the necessary set of state-action pairs to perform IRL. Then, to approximate the action likelihood for a goal $g_j$ at state $s_{i,t}$, we compute the difference between action $a_t$ and the action calculated from the controller at state $s_{i,t}$ to go to $g_i$ as follows:
\begin{equation}
 Pr(a_t|s_{i,t},g_j) \propto e^{-\alpha \lVert a_t - a_{CL} \rVert_{2}}
\end{equation}
Where $a_{CL}$ is the action given by the system's closed loop controller to go from state $s_{i,t}$ to goal $g_j$.
\section{Simulations}\label{sec:simulations}
We perform a set of simulations having an Unmanned Aerial Vehicle (UAV) navigating in a 2-D stochastic environment to a desired location. The goal of the simulation is to show the following:
\begin{itemize}
    \item An attacker can lead an autonomous vehicle to an undesired destination by spoofing its sensors while hiding the attack vectors within the noise profiles the sensors or the environment uncertainty model.
    \item Algorithm \ref{alg:alg1} manages to infer the intention of the attacker and the set of compromised sensors.
    \item The effect of applying Active Exploration approach to Algorithm \ref{alg:alg1}
\end{itemize}
\subsection{UAV Model}
\subsection{UAV Control}
\subsection{Simulations Environment}
 We represent the discretized state space of the environment as an occupancy grid which maps the environment to a 2-D matrix of cells. Each cell represents a state and holds a probability value that indicates the probability of the vehicle reaching this cell by taking a certain action.
In normal operation conditions (No attack detected), the UAV takes an action at each cell according to its mission optimal policy $\pi*$. The set of actions $A$ that the UAV can take at any cell is defined as follows:
$A =$ \textit{\{move forwards \{F\}, move backwards \{B\}, move right \{R\}, move left \{L\}\}}.
Each action is mapped to an altitude and position control input to the UAV to make it move from one cell to the next desired cell. As the environment is stochastic, we define the transition probability function $T(s,s',a)$ that maps the disturbances in the environment to a probability associated with taking each action $a$ to go to state $s'$ from state $s$.
\subsection{Simulation Results}
\subsubsection{Simulation Setup}
We use a 10mx10m occupancy grid to conduct the simulations. We define function $R$ as a function of the cell (state) only. $R(s) = 100$ for the desired destination and $R(s) = -3$ for any other cell. Figure. \ref{fig:sim-env} illustrates how we define the reward function for each state and the transition probability function for each state-action pair. The UAV uses two sensors to estimate its state. State estimation is done by a kalman filter that fuses the two sensor readings and prorgices the state estimate to the controller. The controller calculates the optimal action that maximizes the UAV's expected future rewards, driving it to the desired goal.
\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{sim-env}
\caption{An example of the discretized environment. The green cell represents the desired destination of the UAV with a reward value of 100. The penalty of moving from one cell to another is represented by a -3 reward. If the vehicle moves to the right, it has 0.8 chance of ending up in the desired cell. and 0.2 chance of ending up in an adjacent cell.}
 \label{fig:sim-env}
\end{figure}
\subsubsection{Sensor Spoofing Attack}
\begin{figure}[]
	\centering
	\subfigure[] {\includegraphics[width=0.23\textwidth]{fig/good-goal-policy}}
	\subfigure[] {\includegraphics[width=0.23\textwidth]{fig/bad-goal-policy}}
	%\vspace{-5pt}
	\caption{(a) Optimal actions to go to the desired destination (b) Optimal actions to go to the attacker's desired destination}
	\label{fig:policies}
%	\vspace{-1pt}
\end{figure}
We simulate the case where the an attacker is spoofing one of the two sensor the UAV has. We assume the attacker has full knowledge of the system and exploits the uncertainty in the system model to hide the attack vectors which means that both sensor readings will comply the system model and we cannot determine which sensor is compromised by just comparing the sequence of sensor readings to the dynamical model of the system. The goal of the attack is to make the UAV navigate towards the attacker's desired goal optimally. Figure. \ref{fig:policies} illustrates the difference between the mission policy of the UAV and the attacker policy. The arrow at each cell present the optimal action associated with that cell.

As shown in Figures \ref{fig:successful-attack} (a-h), the sequence of the compromised sensor readings comply with the system model and it drags the estimated state of the UAV towards the cells associated with optimal actions that lead towards the attacker's goal instead of the mission's goal. The UAV can detect from the divergence of the two sensor readings that one of them is compromised but it cannot determine which one is being spoofed and ends up navigating to the attacker's goal.

\subsubsection{Attacker Intention Inference}
We show how the Attacker Intention Inference algorithm works as follows: We apply the same attack we illustrated in the previous section. As shown in Figures \ref{fig:recovered-attack}, the attack gets detected at snapshot (a) and the Attacker Intention Inference algorithms starts to run. We assume in this environment that the set of all undesired goals is the set of all boarder cells of the grid. We assume we don't have any initial knowledge about where the attacker's goal is, which means the prior takes the form of a uniform distribution initially. Snapshots (b-e) show the convergence of the posterior to the region of the attacker's goal. At snapshot (f), the variance of the estimated posterior of the attacker's goal goes below the threshold value which activates the recovery process. The compromised sensor is identified and is discarded from the calculations of the state estimation
\begin{figure*}[]
	\centering
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_successfull/1}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_successfull/2}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_successfull/4}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_successfull/6}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_successfull/8}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_successfull/10}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_successfull/11}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_successfull/13}}
	\vspace{-5pt}
	\caption{Red cells represent the readings from the compromised sensor. Yellow cells represent estimated states (a) UAV is not under attack. (b-g) Attack is being applied hiding the attack vector within the system uncertainty bounds. (h) The UAV eventually reaches the undesired destination.}
	\label{fig:successful-attack}
%	\vspace{-1pt}
\end{figure*}

\begin{figure*}[]
	\centering
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_recovery/1}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_recovery/2}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_recovery/3}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_recovery/4}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_recovery/5}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_recovery/6}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_recovery/7}}
	\subfigure[] {\includegraphics[width=0.15\textwidth]{fig/attack_recovery/8}}
	\vspace{-5pt}
	\caption{Sensor attack sequence is applied as in Figure. \ref{fig:successful-attack}. (a-e) Sequence updates of attack intention inference for the compromised sensor. (f-h) The inference confidence level exceeds the threshold, the compromised sensor is discarded and the UAV recovers from the attack.}
	\label{fig:recovered-attack}
%	\vspace{-1pt}
\end{figure*}
\section{Conclusions \& Future Work}\label{sec:conclusion}
We showed that, for cyber physical systems under sensor spoofing attacks, using Inverse Reinforcement Learning based method can predict both the intention of the attacker and the spoofed sensors. Our simulation results show that applying our proposed method is able to predict the goal of an attacker that hides his attack within the uncertainty bounds of the physical system before the system reaches the attacker's desired state, and uses this prediction information to recover the system from such an attack.
% conference papers do not normally have an appendix
% use section* for acknowledgment
%\vspace{-15pt}
\section*{Acknowledgment}
% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\bibliographystyle{IEEEtran}
\bibliography{mendeley,bibliography}


\end{document}
