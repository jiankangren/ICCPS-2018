@article{Xia2016,
abstract = {Designing intelligent and robust autonomous navigation systems remains a great challenge in mobile robotics. Inverse reinforcement learning (IRL) offers an efficient learning technique from expert demonstrations to teach robots how to perform specific tasks without manually specifying the reward function. Most of existing IRL algorithms assume the expert policy to be optimal and deterministic, and are applied to experiments with relatively small-size state spaces. However, in autonomous navigation tasks, the state spaces are frequently large and demonstrations can hardly visit all the states. Meanwhile the expert policy may be non-optimal and stochastic. In this paper, we focus on IRL with large-scale and high-dimensional state spaces by introducing the neural network to generalize the expert's behaviors to unvisited regions of the state space and an explicit policy representation is easily expressed by neural network, even for the stochastic expert policy. An efficient and convenient algorithm, Neural Inverse Reinforcement Learning (NIRL), is proposed. Experimental results on simulated autonomous navigation tasks show that a mobile robot using our approach can successfully navigate to the target position without colliding with unpredicted obstacles, largely reduce the learning time, and has a good generalization performance on undemonstrated states. Hence prove the robot intelligence of autonomous navigation transplanted from limited demonstrations to completely unknown tasks.},
author = {Xia, Chen and {El Kamel}, Abdelkader},
doi = {10.1016/j.robot.2016.06.003},
file = {:C$\backslash$:/papers/Neural inverse reinforcement learning in autonomous navigation.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous navigation,Dynamic environments,Inverse reinforcement learning,Learning from demonstration,Markov decision processes,Neural network},
pages = {1--14},
publisher = {Elsevier B.V.},
title = {{Neural inverse reinforcement learning in autonomous navigation}},
url = {http://dx.doi.org/10.1016/j.robot.2016.06.003},
volume = {84},
year = {2016}
}
@article{Abbeel2004,
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our. algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
archivePrefix = {arXiv},
arxivId = {1206.5264},
author = {Abbeel, Pieter and Ng, Andrew Y.},
doi = {10.1145/1015330.1015430},
eprint = {1206.5264},
file = {:C$\backslash$:/papers/icml04-apprentice.pdf:pdf},
isbn = {1581138285},
journal = {Twenty-first international conference on Machine learning  - ICML '04},
pages = {1},
title = {{Apprenticeship learning via inverse reinforcement learning}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015430},
year = {2004}
}
@article{Fawzi2014,
abstract = {The vast majority of today's critical infrastructure is supported by numerous feedback control loops and an attack on these control loops can have disastrous consequences. This is a major concern since modern control systems are becoming large and decentralized and thus more vulnerable to attacks. This paper is concerned with the estimation and control of linear systems when some of the sensors or actuators are corrupted by an attacker. In the first part we look at the estimation problem where we characterize the resilience of a system to attacks and study the possibility of increasing its resilience by a change of parameters. We then propose an efficient algorithm to estimate the state despite the attacks and we characterize its performance. Our approach is inspired from the areas of error-correction over the reals and compressed sensing. In the second part we consider the problem of designing output-feedback controllers that stabilize the system despite attacks. We show that a principle of separation between estimation and control holds and that the design of resilient output feedback controllers can be reduced to the design of resilient state estimators.},
archivePrefix = {arXiv},
arxivId = {1205.5073},
author = {Fawzi, Hamza and Tabuada, Paulo and Diggavi, Suhas},
doi = {10.1109/TAC.2014.2303233},
eprint = {1205.5073},
file = {:C$\backslash$:/papers/Secure Estimation and Control for Cyber-Physical.pdf:pdf},
isbn = {0018-9286},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {Algorithm,feedback controller},
number = {6},
pages = {1454--1467},
title = {{Secure estimation and control for cyber-physical systems under adversarial attacks}},
volume = {59},
year = {2014}
}
@article{Pajic2017,
author = {Pajic, Miroslav and Weimer, James and Bezzo, Nicola and Sokolsky, Oleg and Pappas, George J and Lee, Insup and Pajic, P O C M},
doi = {10.1109/MCS.2016.2643239},
file = {:C$\backslash$:/papers/Design and Implementation of attack-resilient cyberphysical systems.pdf:pdf},
issn = {1066-033X},
journal = {IEEE Control Systems},
number = {2},
pages = {66--81},
title = {{Design and Implementation of Attack-Resilient Cyber-Physical Systems}},
year = {2017}
}
@article{Levine2014,
abstract = {We present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems. These tra-jectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization. Our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous. We show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques. We present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with nu-merous contact discontinuities and underactuation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.05611v1},
author = {Levine, Sergey and Abbeel, Pieter},
doi = {10.1109/ICRA.2015.7138994},
eprint = {arXiv:1501.05611v1},
file = {:C$\backslash$:/papers/Learning Neural Network Policies with Guided Policy.pdf:pdf},
isbn = {9781479969227},
journal = {Advances in Neural Information Processing Systems 27},
pages = {1--3},
title = {{Learning Dynamic Manipulation Skills under Unknown Dynamics with Guided Policy Search}},
url = {http://papers.nips.cc/paper/5444-learning-neural-network-policies-with-guided-policy-search-under-unknown-dynamics.pdf},
year = {2014}
}
@article{Pajic2014,
abstract = {The interaction between information technology and physical world makes Cyber-Physical Systems (CPS) vulnerable to malicious attacks beyond the standard cyber attacks. This has motivated the need for attack-resilient state estimation. Yet, the existing state-estimators are based on the non-realistic assumption that the exact system model is known. Consequently, in this work we present a method for state estimation in presence of attacks, for systems with noise and modeling errors. When the the estimated states are used by a state-based feedback controller, we show that the attacker cannot destabilize the system by exploiting the difeerence between the model used for the state estimation and the real physical dynamics of the system. Furthermore, we describe how implementation issues such as jitter, latency and synchronization errors can be mapped into parameters of the state estimation procedure that describe modeling errors, and provide a bound on the state-estimation error caused by modeling errors. This enables mapping control performance requirements into real-time (i.e., timing related) specifications imposed on the underlying platform. Finally, we illustrate and experimentally evaluate this approach on an unmanned ground vehicle case-study. ABSTRACT The interaction between information technology and phys-ical world makes Cyber-Physical Systems (CPS) vulner-able to malicious attacks beyond the standard cyber at-tacks. This has motivated the need for attack-resilient state estimation. Yet, the existing state-estimators are based on the non-realistic assumption that the exact system model is known. Consequently, in this work we present a method for state estimation in presence of attacks, for systems with noise and modeling errors. When the the estimated states are used by a state-based feedback controller, we show that the attacker cannot destabilize the system by exploiting the difference be-tween the model used for the state estimation and the real physical dynamics of the system. Furthermore, we describe how implementation issues such as jitter, la-tency and synchronization errors can be mapped into parameters of the state estimation procedure that de-scribe modeling errors, and provide a bound on the state-estimation error caused by modeling errors. This enables mapping control performance requirements into real-time (i.e., timing related) specifications imposed on * This material is based on research sponsored by DARPA under agreement number FA8750-12-2-0247. The U.S. Gov-ernment is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorse-ments, either expressed or implied, of DARPA or the U.S. Government. the underlying platform. Finally, we illustrate and ex-perimentally evaluate this approach on an unmanned ground vehicle case-study.},
author = {Pajic, Miroslav and Weimer, James and Bezzo, Nicola and Tabuada, Paulo and Sokolsky, Oleg and Lee, Insup and Pappas, George and {Weimer Nicola Bezzo}, James and {Sokolsky Insup Lee}, Oleg and Pappas, George J},
doi = {10.1109/ICCPS.2014.6843720},
file = {:C$\backslash$:/papers/Robustness of Attack-resilient State Estimators.pdf:pdf},
isbn = {9781479949304},
keywords = {C3 [Special-purpose and Application-based Systems],Management of Computing and Information Systems,Process control systems,Security and Protection,Security and Protection—Unauthorized access,Special-purpose and Application-based Systems,Unauthorized access},
number = {April},
pages = {163--174},
title = {{Robustness of Attack-Resilient State Estimators Recommended Citation Robustness of Attack-Resilient State Estimators Robustness of Attack-resilient State Estimators *}},
url = {http://repository.upenn.edu/cis{\_}papers{\%}0Ahttp://repository.upenn.edu/cis{\_}papers/771{\%}0Ahttp://dx.doi.org/10.1109/ICCPS.2014.6843720},
year = {2014}
}
@article{Psiaki2016,
author = {Psiaki, Mark L and Humphreys, Todd E},
file = {:C$\backslash$:/papers/Attackers can spoof navigation.pdf:pdf},
journal = {IEEE Spectrum},
number = {8},
pages = {26--53},
title = {{GPS Lies}},
url = {http://spectrum.ieee.org/telecom/wireless/what-5g-engineers-can-learn-from-radio-interferences-troubled-past{\%}5Cnhttp://ieeexplore.ieee.org/document/7524168/},
volume = {53},
year = {2016}
}
@article{Manzoor2016,
abstract = {Learning motion prediction of human pedestrian is an important area of research in Human-Robot Interaction (HRI). Traditionally, trajectories are used in Inverse Reinforcement Learning (IRL) framework. However, it is observed that available datasets have very low human-human interaction and usually confined to a given scenario. This may lead to learning sub-optimal behavior. A simulation based approach is presented where motion model is used to generate pedestrian behavior. Unlike simulator, robot has limited state observability. Therefore, idea is to learn motion model in low dimensional robot space. Instead of using output trajectories and matching observed feature count; a new approach is presented in which expected reward is learnt through social force-field generated by social force model (SFM) inspired expert. It is observed that in this case algorithm has fast convergence and improved pedestrian intention prediction.},
author = {Manzoor, Mohsin and Kunwar, F.},
doi = {10.1109/ICRAI.2016.7791244},
file = {:C$\backslash$:/papers/Learn Pedestrian Navigation from Expert.pdf:pdf},
isbn = {9781509040599},
journal = {2016 2nd International Conference on Robotics and Artificial Intelligence, ICRAI 2016},
keywords = {Apprenticeship,IRL,SFM},
pages = {147--151},
title = {{Learn pedestrian navigation from expert}},
year = {2016}
}
@article{Murguia2016,
author = {Murguia, Carlos and Ruths, Justin},
doi = {10.1109/CCA.2016.7587875},
file = {:C$\backslash$:/papers/CUSUM and Chi-squared Attack Detection of Compromised Sensors.pdf:pdf},
isbn = {978-1-5090-0755-4},
journal = {2016 IEEE Conference on Control Applications (CCA)},
pages = {474--480},
title = {{CUSUM and chi-squared attack detection of compromised sensors}},
url = {http://ieeexplore.ieee.org/document/7587875/},
year = {2016}
}
@article{Baranes2013,
abstract = {We introduce the Self-Adaptive Goal Generation Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsically motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters. We present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: (1) learning the inverse kinematics in a highly-redundant robotic arm, (2) learning omnidirectional locomotion with motor primitives in a quadruped robot, and (3) an arm learning to control a fishing rod with a flexible wire. We show that (1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; (2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; (3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot. ?? 2012 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1301.4862},
author = {Baranes, Adrien and Oudeyer, Pierre Yves},
doi = {10.1016/j.robot.2012.05.008},
eprint = {1301.4862},
file = {:C$\backslash$:/papers/Active Learning of Inverse Models.pdf:pdf},
isbn = {0921889012000},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Active learning,Autonomous motor learning,Competence based intrinsic motivation,Curiosity-driven task space exploration,Developmental robotics,Goal babbling,Inverse models,Motor development},
number = {1},
pages = {49--73},
title = {{Active learning of inverse models with intrinsically motivated goal exploration in robots}},
volume = {61},
year = {2013}
}
@article{Sabaliauskaite2017,
author = {Sabaliauskaite, Giedre and Ng, Geok See and Ruths, Justin and Mathur, Aditya},
doi = {10.1109/PRDC.2017.47},
file = {:C$\backslash$:/papers/Comparison of Corrupted Sensor Data Detection Methods.pdf:pdf},
isbn = {978-1-5090-5652-1},
journal = {2017 IEEE 22nd Pacific Rim International Symposium on Dependable Computing (PRDC)},
pages = {235--244},
title = {{Comparison of Corrupted Sensor Data Detection Methods in Detecting Stealthy Attacks on Cyber-Physical Systems}},
url = {http://ieeexplore.ieee.org/document/7920627/},
year = {2017}
}
@article{Michini2013,
abstract = {Reward learning from demonstration is the task of inferring the intents or goals of an agent demonstrating a task. Inverse reinforcement learning methods utilize the Markov decision process (MDP) framework to learn rewards, but typically scale poorly since they rely on the calculation of optimal value functions. Several key modifications are made to a previously developed Bayesian nonparametric inverse reinforcement learning algorithm that avoid calculation of an optimal value function and no longer require discretization of the state or action spaces. Experimental results given demonstrate the ability of the resulting algorithm to scale to larger problems and learn in domains with continuous demonstrations.},
author = {Michini, Bernard and Cutler, Mark and How, Jonathan P.},
doi = {10.1109/ICRA.2013.6630592},
file = {:C$\backslash$:/papers/Scalable Reward Learning from Demonstration.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
journal = {In Proceedings of the IEEE International Conference on Robotics and Automation},
pages = {303--308},
title = {{Scalable reward learning from demonstration}},
year = {2013}
}
@article{Michini2015,
author = {Michini, B},
file = {:C$\backslash$:/papers/Bayesian Nonparametric Reward Learning.pdf:pdf},
journal = {{\ldots} IEEE Transactions on},
title = {{Bayesian nonparametric reward learning from demonstration}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=7076638},
year = {2015}
}
@article{Michini2011,
author = {Michini, B and How, J P},
file = {:C$\backslash$:/papers/A Human-Interactive Course of Action Planner for Aircraft Carrier Deck Operations.pdf:pdf},
isbn = {9781600869440},
journal = {AIAA Infotech@Aerospace 2011},
pages = {1--11},
title = {{A Human-Interactive Course of Action Planner for Aircraft Carrier Deck Operations}},
year = {2011}
}
@article{Ivanov2014,
author = {Ivanov, Radoslav and Lee, Insup},
file = {:C$\backslash$:/papers/Attack-Resilient Sensor Fusion.pdf:pdf},
number = {February},
pages = {1--6},
title = {{Attack-Resilient Sensor Fusion Attack-Resilient Sensor Fusion}},
year = {2014}
}
@article{Humphreys2008,
abstract = {A portable civilian GPS spoofer is implemented on a digital signal processor and used to characterize spoofing effects and develop defenses against civilian spoofing. This work is intended to equip GNSS users and receiver manufacturers with authentication methods that are effective against unsophisticated spoofing attacks. The work also serves to refine the civilian spoofing threat assessment by demonstrating the challenges involved in mounting a spoofing attack.},
author = {Humphreys, Todd E. and Ledvina, Brent M and Tech, Virginia and Psiaki, Mark L and Hanlon, Brady W O and Kintner, Paul M},
file = {:C$\backslash$:/papers/Assessing the Spoofing Threat.pdf:pdf},
isbn = {9781605606897},
journal = {Proceedings of the 21st International Technical Meeting of the Satellite Division of The Institute of Navigation},
number = {September 2008},
pages = {2314--2325},
title = {{Assessing the Spoofing Threat: Development of a Portable GPS Civilian Spoofer}},
year = {2008}
}
@article{Michini2012,
abstract = {Inverse reinforcement learning (IRL) is the task of learning the reward function of a Markov Decision Process (MDP) given knowledge of the transition function and a set of expert demonstrations. While many IRL algorithms exist, Bayesian IRL [1] provides a general and principled method of reward learning by casting the problem in the Bayesian inference framework. However, the algorithm as originally presented suffers from several inefficiencies that prohibit its use for even moderate problem sizes. This paper proposes modifications to the original Bayesian IRL algorithm to improve its efficiency and tractability in situations where the state space is large and the expert demonstrations span only a small portion of it. The key insight is that the inference task should be focused on states that are similar to those encountered by the expert, as opposed to making the naive assumption that the expert demonstrations contain enough information to accurately infer the reward function over the entire state space. A modified algorithm is presented and experimental results show substantially faster convergence while maintaining the solution quality of the original method.},
author = {Michini, Bernard and How, Jonathan P.},
doi = {10.1109/ICRA.2012.6225241},
file = {:C$\backslash$:/papers/Improving the Efficiency of Bayesian Inverse Reinforcement Learni.pdf:pdf},
isbn = {978-1-4673-1405-3},
issn = {10504729},
journal = {IEEE International Conference on Robotics and Automation},
pages = {3651--3656},
title = {{Improving the efficiency of Bayesian inverse reinforcement learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6225241},
year = {2012}
}
@article{Guez,
author = {Guez, Arthur and Silver, David and Dayan, Peter},
file = {:C$\backslash$:/papers/Efficient Bayes-Adaptive Reinforcement Learning.pdf:pdf},
pages = {1--9},
title = {{Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search}}
}
@article{Dimitrakakis2012,
abstract = {We generalise the problem of inverse reinforcement learning to multiple tasks, from multiple demonstrations. Each one may represent one expert trying to solve a different task, or as different experts trying to solve the same task. Our main contribution is to formalise the problem as statistical preference elicitation, via a number of structured priors, whose form captures our biases about the relatedness of different tasks or expert policies. In doing so, we introduce a prior on policy optimality, which is more natural to specify. We show that our framework allows us not only to learn to efficiently from multiple experts but to also effectively differentiate between the goals of each. Possible applications include analysing the intrinsic motivations of subjects in behavioural experiments and learning from multiple teachers.},
archivePrefix = {arXiv},
arxivId = {1106.3655},
author = {Dimitrakakis, Christos and Rothkopf, Constantin A.},
doi = {10.1007/978-3-642-29946-9_27},
eprint = {1106.3655},
file = {:C$\backslash$:/papers/Bayesian multitask inverse reinforcement.pdf:pdf},
isbn = {9783642299452},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Bayesian inference,intrinsic motivations,inverse reinforcement learning,multitask learning,preference elicitation},
pages = {273--284},
title = {{Bayesian multitask inverse reinforcement learning}},
volume = {7188 LNAI},
year = {2012}
}
@article{Qifeng2011,
abstract = {We present new algorithms for inverse reinforcement learning (IRL, or inverse optimal control) in convex optimization settings. We argue that finite-space IRL can be posed as a convex quadratic program under a Bayesian inference framework with the objective of maximum a posteriori estimation. To deal with problems in large or even infinite state space, we propose a Gaussian process model and use preference graphs to represent observations of decision trajectories. Our method is distinguished from other approaches to IRL in that it makes no assumptions about the form of the reward function and yet it retains the promise of computationally manageable implementations for potential real-world applications. In comparison with an establish algorithm on small-scale numerical problems, our method demonstrated better accuracy in apprenticeship learning and a more robust dependence on the number of observations.},
archivePrefix = {arXiv},
arxivId = {1208.2112},
author = {Qifeng, Qiao and Beling, P A},
doi = {10.1109/ACC.2011.5990948},
eprint = {1208.2112},
file = {:C$\backslash$:/papers/Inverse Reinforcement Learning with Gaussian Process.pdf:pdf},
isbn = {0743-1619},
issn = {07431619},
journal = {2011 American Control Conference (ACC 2011)},
keywords = {Accuracy,Approximation methods,Bayesian inference framework,Bayesian methods,Gaussian process model,Gaussian processes,IRL,Machine learning,Markov processes,Trajectory,apprenticeship learning,belief networks,convex optimization settings,convex programming,inference mechanisms,inverse reinforcement learning,learning (artificial intelligence),maximum a posteriori estimation,maximum likelihood estimation,quadratic programming,small-scale numerical problems},
pages = {113--118},
title = {{Inverse reinforcement learning with Gaussian process}},
url = {http://ieeexplore.ieee.org/ielx5/5975310/5989965/05990948.pdf?tp={\&}arnumber=5990948{\&}isnumber=5989965},
year = {2011}
}
@article{Yokoyama2017,
author = {Yokoyama, Nobuhiro},
doi = {10.2514/6.2017-1254},
file = {:C$\backslash$:/Users/Mahmoud/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yokoyama - 2017 - Inference of Aircraft Intent via Inverse Optimal Control Including Second-Order Optimality Condition.pdf:pdf;:C$\backslash$:/papers/Inference of Aircraft Intent via Inverse Optimal Control.pdf:pdf},
isbn = {978-1-62410-450-3},
journal = {AIAA Guidance, Navigation, and Control Conference},
number = {January},
pages = {1--16},
title = {{Inference of Aircraft Intent via Inverse Optimal Control Including Second-Order Optimality Condition}},
url = {http://arc.aiaa.org/doi/10.2514/6.2017-1254},
year = {2017}
}
@article{Michini,
author = {Michini, Bernard and How, Jonathan P},
file = {:C$\backslash$:/papers/Bayesian Nonparametric Inverse Reinforcement Learning.pdf:pdf},
title = {{Bayesian Nonparametric Inverse Reinforcement Learning}}
}
@article{Lewis2012,
abstract = {This article describes the use of principles of reinforcement learning to design feedback controllers for discrete- and continuous-time dynamical systems that combine features of adaptive control and optimal control. Adaptive control [1], [2] and optimal control [3] represent different philosophies for designing feedback controllers. Optimal controllers are normally designed of ine by solving Hamilton JacobiBellman (HJB) equations, for example, the Riccati equation, using complete knowledge of the system dynamics. Determining optimal control policies for nonlinear systems requires the offline solution of nonlinear HJB equations, which are often difficult or impossible to solve. By contrast, adaptive controllers learn online to control unknown systems using data measured in real time along the system trajectories. Adaptive controllers are not usually designed to be optimal in the sense of minimizing user-prescribed performance functions. Indirect adaptive controllers use system identification techniques to first identify the system parameters and then use the obtained model to solve optimal design equations [1]. Adaptive controllers may satisfy certain inverse optimality conditions [4].},
author = {Lewis, Frank and Vrabie, Draguna and Vamvoudakis, Kyriakos},
doi = {10.1109/MCS.2012.2214134},
file = {:C$\backslash$:/papers/RL{\_}control.pdf:pdf},
issn = {1066-033X},
journal = {IEEE Control Systems},
number = {6},
pages = {76--105},
title = {{Reinforcement Learning and Feedback Control: Using Natural Decision Methods to Design Optimal Adaptive Controllers}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6315769},
volume = {32},
year = {2012}
}
@article{Ziebart2009,
abstract = {We present a novel approach for determining robot movements that efficiently accomplish the robot's tasks while not hindering the movements of people within the environment. Our approach models the goal-directed trajectories of pedestrians using maximum entropy inverse optimal control. The advantage of this modeling approach is the generality of its learned cost function to changes in the environment and to entirely different environments. We employ the predictions of this model of pedestrian trajectories in a novel incremental planner and quantitatively show the improvement in hindrance-sensitive robot trajectory planning provided by our approach.},
author = {Ziebart, Brian D. and Ratliff, Nathan and Gallagher, Garratt and Mertz, Christoph and Peterson, Kevin and Bagnell, J. Andrew and Hebert, Martial and Dey, Anind K. and Srinivasa, Siddhartha},
doi = {10.1109/IROS.2009.5354147},
file = {:C$\backslash$:/papers/Planning-based Prediction for Pedestrians.pdf:pdf},
isbn = {9781424438044},
issn = {1424438039},
journal = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
pages = {3931--3936},
title = {{Planning-based prediction for pedestrians}},
year = {2009}
}
@article{Michini2012a,
abstract = {Inverse reinforcement learning (IRL) is the task of learning the reward function of a Markov Decision Process (MDP) given the tran-sition function and a set of observed demonstrations in the form of state-action pairs. Current IRL algorithms attempt to find a single reward function which explains the entire observation set. In practice, this leads to a computationally-costly search over a large (typically infinite) space of complex reward functions. This paper proposes the notion that if the observations can be partitioned into smaller groups, a class of much sim-pler reward functions can be used to explain each group. The proposed method uses a Bayesian nonparametric mixture model to automatically partition the data and find a set of simple reward functions correspond-ing to each partition. The simple rewards are interpreted intuitively as subgoals, which can be used to predict actions or analyze which states are important to the demonstrator. Experimental results are given for simple examples showing comparable performance to other IRL algorithms in nominal situations. Moreover, the proposed method handles cyclic tasks (where the agent begins and ends in the same state) that would break existing algorithms without modification. Finally, the new algorithm has a fundamentally different structure than previous methods, making it more computationally efficient in a real-world learning scenario where the state space is large but the demonstration set is small.},
author = {Michini, Bernard and {Jonathan P. How}},
doi = {10.1007/978-3-642-33486-3},
file = {:C$\backslash$:/papers/Bayesian Nonparametric IRL.pdf:pdf},
isbn = {978-3-642-33485-6},
issn = {03029743},
journal = {Ecmlpkdd},
pages = {148--163},
title = {{Machine Learning and Knowledge Discovery in Databases}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-33486-3{\%}5Cnhttp://link.springer.com/10.1007/978-3-642-33486-3},
volume = {7524},
year = {2012}
}
@article{Finn2016,
abstract = {Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.},
archivePrefix = {arXiv},
arxivId = {1603.00448},
author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
eprint = {1603.00448},
file = {:C$\backslash$:/papers/Guided Cost Learning-Deep Inverse Optimal Control via Policy Optimization.pdf:pdf},
isbn = {9781510829008},
title = {{Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization}},
url = {http://arxiv.org/abs/1603.00448},
volume = {48},
year = {2016}
}
@article{Ramachandran2007,
abstract = {Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.},
author = {Ramachandran, Deepak and Amir, Eyal},
file = {:C$\backslash$:/papers/Bayesian Inverse Reinforcement Learning.pdf:pdf},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Markov-decision processes,reinforcement learning},
pages = {2586--2591},
title = {{Bayesian inverse reinforcement learning}},
year = {2007}
}
@article{Qin2004,
abstract = {Correlating and analyzing security alerts is a critical and challenging task in security management. Recently, some techniques have been proposed for security alert correlation. However, these approaches focus more on basic or low-level alert correlation. In this paper, we study how to conduct probabilistic inference to correlate and analyze attack scenarios. Specifically, we propose an approach to solving the following problems: 1) How to correlate isolated attack scenarios resulted from low-level alert correlation? 2) How to identify attacker's high-level strategies and intentions? 3) How to predict the potential attacks based on observed attack activities? We evaluate our approaches using DARPA's grand challenge problem (GCP) data set. The results demonstrate the capability of our approach in correlating isolated attack scenarios, identifying attack strategies and predicting future attacks.},
author = {Qin, Xinzhou and Lee, Wenke},
doi = {10.1109/CSAC.2004.7},
file = {:C$\backslash$:/papers/Attack Plan Recognition and Prediction Using Causal Networks.pdf:pdf},
isbn = {0769522521},
issn = {10639527},
journal = {Proceedings - Annual Computer Security Applications Conference, ACSAC},
keywords = {Alert correlation,Attack scenario analysis,Intrusion detection,Security management},
pages = {370--379},
title = {{Attack plan recognition and prediction using causal networks}},
year = {2004}
}
@article{Ng2000,
abstract = {This paper addresses the problem of inverse reinforcement learning (IRL) in Markov decision processes, that is, the problem of extracting a reward function given observed, optimal behaviour. IRL may be useful for apprenticeship learning to acquire skilled behaviour, and for ascertaining the reward function being optimized by a natural system. We rst characterize the set of all reward functions for which a given policy is optimal. We then derive three algorithms for IRL. The rst two deal with the case where the entire policy is known; we handle tabulated reward functions on a nite state space and linear functional approximation of the reward function over a potentially in- nite state space. The third algorithm deals with the more realistic case in which the policy is known only through a nite set of observed trajectories. In all cases, a key issue is degeneracythe existence of a large set of reward functions for which the observed policy is optimal. To remove...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ng, Andrew and Russell, Stuart},
doi = {10.2460/ajvr.67.2.323},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/papers/Algorihms For Inverse Reinforcement Learning.pdf:pdf},
isbn = {1-55860-707-2},
issn = {00029645},
journal = {Proceedings of the Seventeenth International Conference on Machine Learning},
pages = {663--670},
pmid = {16454640},
title = {{Algorithms for inverse reinforcement learning}},
url = {http://www-cs.stanford.edu/people/ang/papers/icml00-irl.pdf},
volume = {0},
year = {2000}
}
@article{Hadfield-Menell2016,
abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
archivePrefix = {arXiv},
arxivId = {1606.03137},
author = {Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
eprint = {1606.03137},
file = {:C$\backslash$:/papers/Cooperative Inverse Reinforcement Learning.pdf:pdf},
number = {Nips},
title = {{Cooperative Inverse Reinforcement Learning}},
url = {http://arxiv.org/abs/1606.03137},
year = {2016}
}
@article{Zhifei2012,
abstract = {A major challenge faced by machine learning community is the decision making problems under uncertainty. Reinforcement Learning (RL) techniques provide a powerful solution for it. An agent used by RL interacts with a dynamic environment and finds a policy through a reward function, without using target labels like Supervised Learning (SL). However, one fundamental assumption of existing RL algorithms is that reward function, the most succinct representation of the designer's intention, needs to be provided beforehand. In practice, the reward function can be very hard to specify and exhaustive to tune for large and complex problems, and this inspires the development of Inverse Reinforcement Learning (IRL), an extension of RL, which directly tackles this problem by learning the reward function through expert demonstrations. IRL introduces a new way of learning policies by deriving expert's intentions, in contrast to directly learning policies, which can be redundant and have poor generalization ability. In this paper, the original IRL algorithms and its close variants, as well as their recent advances are reviewed and compared.},
author = {Zhifei, Shao and Joo, Er Meng},
doi = {10.1109/CEC.2012.6256507},
file = {:C$\backslash$:/papers/A Review of Inverse Reinforcement Learning.pdf:pdf},
isbn = {9781467315098},
pages = {1--8},
title = {{A review of inverse reinforcement learning theory and recent advances}},
url = {http://ieeexplore.ieee.org/ielx5/6241678/6252855/06256507.pdf?tp={\&}arnumber=6256507{\&}isnumber=6252855{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6256507},
year = {2012}
}
@article{Best2015,
abstract = {Contextual cues can provide a rich source of information for robots that operate in the presence of other agents such as people, animals, vehicles and fellow robots. We are interested in context, in the form of the behavioural intent of an agent, for enhanced trajectory prediction. We present a Bayesian framework that estimates both the intended goal destination and future trajectory of a mobile agent moving among multiple static obstacles. Our method is based on multi-modal hypotheses of the intended goal, and is focused primarily on the long-term trajectory of the agent. We propose a computationally efficient solution and demonstrate its behaviour in a pedestrian scenario with a real-world data set. Results show the benefits of our method in comparison to traditional trajectory prediction methods and illustrate the feasibility of integration with higher-level planning algorithms.},
author = {Best, Graeme and Fitch, Robert},
doi = {10.1109/IROS.2015.7354203},
file = {:C$\backslash$:/papers/Bayesian Intention Inference for Trajectory Prediction.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {5817--5823},
title = {{Bayesian intention inference for trajectory prediction with an unknown goal destination}},
volume = {2015-Decem},
year = {2015}
}
@article{Lopes2009,
abstract = {Previously published cosolvency models are critically evaluated in terms of their ability to mathematically correlate solute solubility in binary solvent mixtures as a function of solvent composition. Computational results show that the accuracy of the models is improved by increasing the number of curve-fit parameters. However, the curve-fit parameters of several models are limited. The combined nearly ideal binary solvent/Redlich-Kister, CNIBS/R-K, was found to be the best solution model in terms of its ability to describe the experimental solubility in mixed solvents. Also resented is an extension of the mixture response surface model. The extension was found to improve the correlational ability of the original model.},
author = {Lopes, Manuel and Melo, Francisco and Montesano, Luis},
doi = {10.1007/978-3-642-04174-7_3},
file = {:C$\backslash$:/papers/Active Learning for Reward Estimation in IRL.pdf:pdf},
isbn = {3642041736},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {31--46},
pmid = {10205606},
title = {{Active learning for reward estimation in inverse reinforcement learning}},
volume = {5782 LNAI},
year = {2009}
}
@article{Qin2004a,
abstract = {Correlating and analyzing security alerts is a critical and challenging task in security management. Recently, some techniques have been proposed for security alert correlation. However, these approaches focus more on basic or low-level alert correlation. In this paper, we study how to conduct probabilistic inference to correlate and analyze attack scenarios. Specifically, we propose an approach to solving the following problems: 1) How to correlate isolated attack scenarios resulted from low-level alert correlation? 2) How to identify attacker's high-level strategies and intentions? 3) How to predict the potential attacks based on observed attack activities? We evaluate our approaches using DARPA's grand challenge problem (GCP) data set. The results demonstrate the capability of our approach in correlating isolated attack scenarios, identifying attack strategies and predicting future attacks.},
author = {Qin, Xinzhou and Lee, Wenke},
doi = {10.1109/CSAC.2004.7},
file = {:C$\backslash$:/papers/acsac{\_}Qin{\_}04.pdf:pdf},
isbn = {0769522521},
issn = {10639527},
journal = {Proceedings - Annual Computer Security Applications Conference, ACSAC},
keywords = {Alert correlation,Attack scenario analysis,Intrusion detection,Security management},
pages = {370--379},
title = {{Attack plan recognition and prediction using causal networks}},
year = {2004}
}
@inproceedings{Abbeel2004a,
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our. algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
archivePrefix = {arXiv},
arxivId = {1206.5264},
author = {Abbeel, Pieter and Ng, Andrew Y.},
booktitle = {Twenty-first international conference on Machine learning  - ICML '04},
doi = {10.1145/1015330.1015430},
eprint = {1206.5264},
isbn = {1581138285},
pages = {1},
title = {{Apprenticeship learning via inverse reinforcement learning}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015430},
year = {2004}
}